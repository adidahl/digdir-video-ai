{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video transcription with OpenAI Whisper\n",
        "\n",
        "This notebook will:\n",
        "- Install and set up OpenAI Whisper\n",
        "- Configure two local video files\n",
        "- Transcribe them with timestamps\n",
        "- Save the transcription (with timelines) to a JSONL file for later use (e.g. vector / graph DB).\n",
        "\n",
        "Whisper docs: [openai/whisper GitHub repo](https://github.com/openai/whisper)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-whisper in ./.venv/lib/python3.13/site-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in ./.venv/lib/python3.13/site-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in ./.venv/lib/python3.13/site-packages (from openai-whisper) (0.62.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from openai-whisper) (2.3.5)\n",
            "Requirement already satisfied: tiktoken in ./.venv/lib/python3.13/site-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from openai-whisper) (2.9.1)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in ./.venv/lib/python3.13/site-packages (from numba->openai-whisper) (0.45.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (3.6)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch->openai-whisper) (2025.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install Whisper (run this once in your environment)\n",
        "# If you are in a virtualenv, make sure it is activated before running this.\n",
        "\n",
        "%pip install -U openai-whisper\n",
        "\n",
        "# On macOS, Whisper also needs ffmpeg installed at the system level.\n",
        "# If you don't have it yet, install via Homebrew in a terminal (NOT in this cell):\n",
        "#   brew install ffmpeg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "ffmpeg found at: /opt/homebrew/bin/ffmpeg\n",
            "Loading Whisper model: large on cpu ...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mffmpeg found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mffmpeg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading Whisper model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m model = \u001b[43mwhisper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/__init__.py:156\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, device, download_root, in_memory)\u001b[39m\n\u001b[32m    154\u001b[39m dims = ModelDimensions(**checkpoint[\u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    155\u001b[39m model = Whisper(dims)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    159\u001b[39m     model.set_alignment_heads(alignment_heads)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2609\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2602\u001b[39m         out = hook(module, incompatible_keys)\n\u001b[32m   2603\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   2604\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2605\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2606\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit should be done inplace.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2607\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2609\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2610\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[32m   2612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2597\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2591\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2592\u001b[39m         child_state_dict = {\n\u001b[32m   2593\u001b[39m             k: v\n\u001b[32m   2594\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2596\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2599\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2600\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2597\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2591\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2592\u001b[39m         child_state_dict = {\n\u001b[32m   2593\u001b[39m             k: v\n\u001b[32m   2594\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2596\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2599\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2600\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "    \u001b[31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2597 (2 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2597\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2591\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2592\u001b[39m         child_state_dict = {\n\u001b[32m   2593\u001b[39m             k: v\n\u001b[32m   2594\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2596\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2599\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2600\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2580\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[32m   2579\u001b[39m     local_metadata[\u001b[33m\"\u001b[39m\u001b[33massign_to_params_buffers\u001b[39m\u001b[33m\"\u001b[39m] = assign\n\u001b[32m-> \u001b[39m\u001b[32m2580\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2581\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2586\u001b[39m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2587\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2588\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module._modules.items():\n\u001b[32m   2590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2487\u001b[39m, in \u001b[36mModule._load_from_state_dict\u001b[39m\u001b[34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[39m\n\u001b[32m   2485\u001b[39m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[32m   2486\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2487\u001b[39m             \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   2489\u001b[39m     action = \u001b[33m\"\u001b[39m\u001b[33mswapping\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcopying\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import whisper\n",
        "\n",
        "# Choose the Whisper model size.\n",
        "# Options include: \"tiny\", \"base\", \"small\", \"medium\", \"large\", \"turbo\".\n",
        "# See model table in the Whisper README: https://github.com/openai/whisper\n",
        "MODEL_NAME = \"large\"  # adjust if you want faster (\"tiny\") or more accurate (\"medium\"/\"large\")\n",
        "\n",
        "# Device selection\n",
        "# NOTE: Whisper on Apple Silicon (MPS) can sometimes produce NaNs.\n",
        "# To keep things stable, we force CPU here. Set FORCE_DEVICE to None to auto-detect.\n",
        "FORCE_DEVICE = \"cpu\"  # options: \"cpu\", \"cuda\", \"mps\", or None for auto\n",
        "\n",
        "if FORCE_DEVICE is not None:\n",
        "    DEVICE = FORCE_DEVICE\n",
        "else:\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = \"cuda\"\n",
        "    elif getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
        "        DEVICE = \"mps\"  # Apple Silicon GPU\n",
        "    else:\n",
        "        DEVICE = \"cpu\"\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Check that ffmpeg is available on the system\n",
        "ffmpeg_path = shutil.which(\"ffmpeg\")\n",
        "if ffmpeg_path is None:\n",
        "    print(\"ffmpeg was NOT found on PATH. Please install it, e.g. with: brew install ffmpeg\")\n",
        "else:\n",
        "    print(f\"ffmpeg found at: {ffmpeg_path}\")\n",
        "\n",
        "print(f\"Loading Whisper model: {MODEL_NAME} on {DEVICE} ...\")\n",
        "model = whisper.load_model(MODEL_NAME, device=DEVICE)\n",
        "print(\"Model loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "video1: exists=True -> /Users/adi/digdir-video-ai/videos/video1.mp4\n",
            "video2: exists=True -> /Users/adi/digdir-video-ai/videos/video2.mp4\n"
          ]
        }
      ],
      "source": [
        "# Configure paths to your two local video files.\n",
        "# Edit these strings to point to the actual files on your Mac.\n",
        "\n",
        "video1_path = Path(\"/Users/adi/digdir-video-ai/videos/video1.mp4\")\n",
        "video2_path = Path(\"/Users/adi/digdir-video-ai/videos/video2.mp4\")\n",
        "\n",
        "videos = [\n",
        "    {\"id\": \"video1\", \"path\": video1_path},\n",
        "    {\"id\": \"video2\", \"path\": video2_path},\n",
        "]\n",
        "\n",
        "for v in videos:\n",
        "    print(f\"{v['id']}: exists={v['path'].exists()} -> {v['path']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Dict, Any\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranscribe_video\u001b[39m(\n\u001b[32m      6\u001b[39m     video_path: Path,\n\u001b[32m      7\u001b[39m     *,\n\u001b[32m      8\u001b[39m     language: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mwhisper.Whisper\u001b[39m\u001b[33m\"\u001b[39m = \u001b[43mmodel\u001b[49m,\n\u001b[32m     10\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transcribe a single video file with Whisper, keeping segment timestamps.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03m    Returns the full Whisper result dict, which includes:\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    - \"text\": full transcript\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    - \"segments\": list of {id, start, end, text, ...}\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTranscribing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "\n",
        "def transcribe_video(\n",
        "    video_path: Path,\n",
        "    *,\n",
        "    language: Optional[str] = None,\n",
        "    model: \"whisper.Whisper\" = model,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Transcribe a single video file with Whisper, keeping segment timestamps.\n",
        "\n",
        "    Returns the full Whisper result dict, which includes:\n",
        "    - \"text\": full transcript\n",
        "    - \"segments\": list of {id, start, end, text, ...}\n",
        "    \"\"\"\n",
        "    print(f\"Transcribing: {video_path}\")\n",
        "    result = model.transcribe(\n",
        "        str(video_path),\n",
        "        language=language,  # set e.g. \"en\" if you know it's English, or leave None for auto-detect\n",
        "        verbose=False,\n",
        "        word_timestamps=False,  # set to True if you want per-word timestamps (requires newer Whisper)\n",
        "    )\n",
        "    print(f\"Transcription finished for {video_path}\")\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcribing: /Users/adi/digdir-video-ai/videos/video1.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adi/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected language: Norwegian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|‚ñè         | 6000/372268 [00:08<09:03, 674.08frames/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m all_segments = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m videos:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     result = \u001b[43mtranscribe_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     segments = result.get(\u001b[33m\"\u001b[39m\u001b[33msegments\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m seg \u001b[38;5;129;01min\u001b[39;00m segments:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtranscribe_video\u001b[39m\u001b[34m(video_path, language, model)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transcribe a single video file with Whisper, keeping segment timestamps.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03mReturns the full Whisper result dict, which includes:\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m- \"text\": full transcript\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m- \"segments\": list of {id, start, end, text, ...}\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTranscribing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# set e.g. \"en\" if you know it's English, or leave None for auto-detect\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mword_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# set to True if you want per-word timestamps (requires newer Whisper)\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTranscription finished for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/transcribe.py:295\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/transcribe.py:201\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    198\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    200\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    207\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/decoding.py:687\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.sample_len):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    690\u001b[39m             i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.no_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    691\u001b[39m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[32m    692\u001b[39m             probs_at_sot = logits[:, \u001b[38;5;28mself\u001b[39m.sot_index].float().softmax(dim=-\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/decoding.py:163\u001b[39m, in \u001b[36mPyTorchInference.logits\u001b[39m\u001b[34m(self, tokens, audio_features)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.initial_token_length:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m     tokens = tokens[:, -\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/model.py:242\u001b[39m, in \u001b[36mTextDecoder.forward\u001b[39m\u001b[34m(self, x, xa, kv_cache)\u001b[39m\n\u001b[32m    239\u001b[39m x = x.to(xa.dtype)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    245\u001b[39m logits = (\n\u001b[32m    246\u001b[39m     x @ torch.transpose(\u001b[38;5;28mself\u001b[39m.token_embedding.weight.to(x.dtype), \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    247\u001b[39m ).float()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/model.py:170\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    169\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/digdir-video-ai/.venv/lib/python3.13/site-packages/whisper/model.py:46\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Transcribe both videos and save transcripts with segment-level timelines to JSONL.\n",
        "\n",
        "output_dir = Path(\"transcripts\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "jsonl_path = output_dir / \"transcripts_segments.jsonl\"\n",
        "all_segments = []\n",
        "\n",
        "for v in videos:\n",
        "    result = transcribe_video(v[\"path\"])\n",
        "    segments = result.get(\"segments\", [])\n",
        "    for seg in segments:\n",
        "        record = {\n",
        "            \"video_id\": v[\"id\"],\n",
        "            \"video_path\": str(v[\"path\"]),\n",
        "            \"segment_id\": seg.get(\"id\"),\n",
        "            \"start\": seg.get(\"start\"),   # seconds from start of video\n",
        "            \"end\": seg.get(\"end\"),       # seconds from start of video\n",
        "            \"text\": seg.get(\"text\"),\n",
        "        }\n",
        "        all_segments.append(record)\n",
        "\n",
        "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for rec in all_segments:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Saved {len(all_segments)} segments to {jsonl_path.resolve()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightrag-hku[api]\n",
            "  Downloading lightrag_hku-1.4.9.8-py3-none-any.whl.metadata (83 kB)\n",
            "Collecting aiohttp (from lightrag-hku[api])\n",
            "  Downloading aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
            "Collecting configparser (from lightrag-hku[api])\n",
            "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting future (from lightrag-hku[api])\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting google-genai<2.0.0,>=1.0.0 (from lightrag-hku[api])\n",
            "  Downloading google_genai-1.52.0-py3-none-any.whl.metadata (46 kB)\n",
            "Collecting json_repair (from lightrag-hku[api])\n",
            "  Downloading json_repair-0.54.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nano-vectordb (from lightrag-hku[api])\n",
            "  Downloading nano_vectordb-0.0.4.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from lightrag-hku[api]) (3.6)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from lightrag-hku[api]) (2.3.5)\n",
            "Collecting pandas<2.4.0,>=2.0.0 (from lightrag-hku[api])\n",
            "  Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
            "Collecting pipmaster (from lightrag-hku[api])\n",
            "  Downloading pipmaster-1.0.10-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydantic (from lightrag-hku[api])\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "Collecting pypinyin (from lightrag-hku[api])\n",
            "  Downloading pypinyin-0.55.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting python-dotenv (from lightrag-hku[api])\n",
            "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from lightrag-hku[api]) (80.9.0)\n",
            "Collecting tenacity (from lightrag-hku[api])\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tiktoken in ./.venv/lib/python3.13/site-packages (from lightrag-hku[api]) (0.12.0)\n",
            "Collecting xlsxwriter>=3.1.0 (from lightrag-hku[api])\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting openai<3.0.0,>=1.0.0 (from lightrag-hku[api])\n",
            "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting aiofiles (from lightrag-hku[api])\n",
            "  Downloading aiofiles-25.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ascii_colors (from lightrag-hku[api])\n",
            "  Downloading ascii_colors-0.11.4-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting asyncpg (from lightrag-hku[api])\n",
            "  Downloading asyncpg-0.31.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
            "Collecting distro (from lightrag-hku[api])\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi (from lightrag-hku[api])\n",
            "  Downloading fastapi-0.122.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting httpcore (from lightrag-hku[api])\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting httpx (from lightrag-hku[api])\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter (from lightrag-hku[api])\n",
            "  Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting passlib[bcrypt] (from lightrag-hku[api])\n",
            "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from lightrag-hku[api]) (7.1.3)\n",
            "Collecting PyJWT<3.0.0,>=2.8.0 (from lightrag-hku[api])\n",
            "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting python-jose[cryptography] (from lightrag-hku[api])\n",
            "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting python-multipart (from lightrag-hku[api])\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pytz (from lightrag-hku[api])\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting uvicorn (from lightrag-hku[api])\n",
            "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting anyio<5.0.0,>=4.8.0 (from google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting google-auth<3.0.0,>=2.14.1 (from google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in ./.venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.0.0->lightrag-hku[api]) (2.32.5)\n",
            "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in ./.venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.0.0->lightrag-hku[api]) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->lightrag-hku[api]) (3.11)\n",
            "Collecting sniffio>=1.1 (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx->lightrag-hku[api]) (2025.11.12)\n",
            "Collecting h11>=0.16 (from httpcore->lightrag-hku[api])\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai<3.0.0,>=1.0.0->lightrag-hku[api]) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas<2.4.0,>=2.0.0->lightrag-hku[api]) (2.9.0.post0)\n",
            "Collecting tzdata>=2022.7 (from pandas<2.4.0,>=2.0.0->lightrag-hku[api])\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->lightrag-hku[api])\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic->lightrag-hku[api])\n",
            "  Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic->lightrag-hku[api])\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.0.0->lightrag-hku[api]) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.0.0->lightrag-hku[api]) (2.5.0)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->lightrag-hku[api])\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<2.4.0,>=2.0.0->lightrag-hku[api]) (1.17.0)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->lightrag-hku[api])\n",
            "  Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.13/site-packages (from ascii_colors->lightrag-hku[api]) (0.2.14)\n",
            "Collecting starlette<0.51.0,>=0.40.0 (from fastapi->lightrag-hku[api])\n",
            "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting annotated-doc>=0.0.2 (from fastapi->lightrag-hku[api])\n",
            "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting bcrypt>=3.1.0 (from passlib[bcrypt]; extra == \"api\"->lightrag-hku[api])\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=21.0 in ./.venv/lib/python3.13/site-packages (from pipmaster->lightrag-hku[api]) (25.0)\n",
            "Collecting ecdsa!=0.15 (from python-jose[cryptography]; extra == \"api\"->lightrag-hku[api])\n",
            "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
            "Collecting cryptography>=3.4.0 (from python-jose[cryptography]; extra == \"api\"->lightrag-hku[api])\n",
            "  Downloading cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
            "Collecting cffi>=2.0.0 (from cryptography>=3.4.0->python-jose[cryptography]; extra == \"api\"->lightrag-hku[api])\n",
            "  Downloading cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
            "Collecting pycparser (from cffi>=2.0.0->cryptography>=3.4.0->python-jose[cryptography]; extra == \"api\"->lightrag-hku[api])\n",
            "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.13/site-packages (from tiktoken->lightrag-hku[api]) (2025.11.3)\n",
            "Collecting click>=7.0 (from uvicorn->lightrag-hku[api])\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading lightrag_hku-1.4.9.8-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_genai-1.52.0-py3-none-any.whl (261 kB)\n",
            "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
            "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
            "Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "Downloading aiofiles-25.1.0-py3-none-any.whl (14 kB)\n",
            "Downloading aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl (489 kB)\n",
            "Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
            "Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
            "Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
            "Downloading ascii_colors-0.11.4-py3-none-any.whl (71 kB)\n",
            "Downloading asyncpg-0.31.0-cp313-cp313-macosx_11_0_arm64.whl (636 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m636.9/636.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
            "Downloading fastapi-0.122.0-py3-none-any.whl (110 kB)\n",
            "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
            "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
            "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "Downloading json_repair-0.54.1-py3-none-any.whl (29 kB)\n",
            "Downloading nano_vectordb-0.0.4.3-py3-none-any.whl (5.6 kB)\n",
            "Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl (495 kB)\n",
            "Downloading pipmaster-1.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading pypinyin-0.55.0-py2.py3-none-any.whl (840 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
            "Downloading cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl (181 kB)\n",
            "Downloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
            "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
            "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Installing collected packages: pytz, passlib, xlsxwriter, websockets, tzdata, typing-inspection, tenacity, sniffio, python-multipart, python-dotenv, pypinyin, PyJWT, pydantic-core, pycparser, pyasn1, propcache, nano-vectordb, multidict, json_repair, jiter, h11, future, frozenlist, ecdsa, distro, configparser, click, cachetools, bcrypt, attrs, asyncpg, ascii_colors, annotated-types, annotated-doc, aiohappyeyeballs, aiofiles, yarl, uvicorn, rsa, pydantic, pyasn1-modules, pipmaster, pandas, httpcore, cffi, anyio, aiosignal, starlette, python-jose, httpx, google-auth, cryptography, aiohttp, openai, google-genai, fastapi, lightrag-hku\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57/57\u001b[0m [lightrag-hku]trag-hku]api]enai]s]t]\n",
            "\u001b[1A\u001b[2KSuccessfully installed PyJWT-2.10.1 aiofiles-25.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.11.0 ascii_colors-0.11.4 asyncpg-0.31.0 attrs-25.4.0 bcrypt-5.0.0 cachetools-6.2.2 cffi-2.0.0 click-8.3.1 configparser-7.2.0 cryptography-46.0.3 distro-1.9.0 ecdsa-0.19.1 fastapi-0.122.0 frozenlist-1.8.0 future-1.0.0 google-auth-2.43.0 google-genai-1.52.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 json_repair-0.54.1 lightrag-hku-1.4.9.8 multidict-6.7.0 nano-vectordb-0.0.4.3 openai-2.8.1 pandas-2.3.3 passlib-1.7.4 pipmaster-1.0.10 propcache-0.4.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pycparser-2.23 pydantic-2.12.4 pydantic-core-2.41.5 pypinyin-0.55.0 python-dotenv-1.2.1 python-jose-3.5.0 python-multipart-0.0.20 pytz-2025.2 rsa-4.9.1 sniffio-1.3.1 starlette-0.50.0 tenacity-9.1.2 typing-inspection-0.4.2 tzdata-2025.2 uvicorn-0.38.0 websockets-15.0.1 xlsxwriter-3.2.9 yarl-1.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install LightRAG (run this once)\n",
        "# This uses the official LightRAG package from https://github.com/HKUDS/LightRAG\n",
        "\n",
        "%pip install \"lightrag-hku[api]\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.13/site-packages (1.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: psycopg2-binary in ./.venv/lib/python3.13/site-packages (2.9.11)\n",
            "Requirement already satisfied: neo4j in ./.venv/lib/python3.13/site-packages (6.0.3)\n",
            "Requirement already satisfied: pytz in ./.venv/lib/python3.13/site-packages (from neo4j) (2025.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install python-dotenv\n",
        "%pip install psycopg2-binary neo4j\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY is set.\n",
            "POSTGRES_DSN set: True\n",
            "NEO4J_URI set: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: [_] Loaded graph from ./lightrag_store/graph_chunk_entity_relation.graphml with 443 nodes, 383 edges\n",
            "INFO:nano-vectordb:Load (443, 1536) data\n",
            "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lightrag_store/vdb_entities.json'} 443 data\n",
            "INFO:nano-vectordb:Load (383, 1536) data\n",
            "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lightrag_store/vdb_relationships.json'} 383 data\n",
            "INFO:nano-vectordb:Load (62, 1536) data\n",
            "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lightrag_store/vdb_chunks.json'} 62 data\n",
            "INFO: [_] Process 42606 KV load full_docs with 1 records\n",
            "INFO: [_] Process 42606 KV load text_chunks with 62 records\n",
            "INFO: [_] Process 42606 KV load full_entities with 1 records\n",
            "INFO: [_] Process 42606 KV load full_relations with 1 records\n",
            "INFO: [_] Process 42606 KV load entity_chunks with 443 records\n",
            "INFO: [_] Process 42606 KV load relation_chunks with 383 records\n",
            "INFO: [_] Process 42606 KV load llm_response_cache with 125 records\n",
            "INFO: [_] Process 42606 doc status load doc_status with 1 records\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightRAG initialized.\n"
          ]
        }
      ],
      "source": [
        "# Configure LightRAG and external store connection settings\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "from lightrag import LightRAG, QueryParam\n",
        "from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\n",
        "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
        "\n",
        "\n",
        "LIGHTRAG_WORK_DIR = \"./lightrag_store\"\n",
        "os.makedirs(LIGHTRAG_WORK_DIR, exist_ok=True)\n",
        "\n",
        "# OpenAI API key (you will set this in your shell, not in the notebook)\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"WARNING: OPENAI_API_KEY is not set. Set it before using LightRAG.\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY is set.\")\n",
        "\n",
        "# Postgres and Neo4j connection details (you will configure these via Docker)\n",
        "POSTGRES_DSN = os.getenv(\"POSTGRES_DSN\")  # e.g. postgresql://user:pass@localhost:5432/dbname\n",
        "NEO4J_URI = os.getenv(\"NEO4J_URI\")        # e.g. bolt://localhost:7687\n",
        "NEO4J_USER = os.getenv(\"NEO4J_USER\")\n",
        "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
        "\n",
        "print(f\"POSTGRES_DSN set: {bool(POSTGRES_DSN)}\")\n",
        "print(f\"NEO4J_URI set: {bool(NEO4J_URI)}\")\n",
        "\n",
        "\n",
        "async def init_lightrag() -> LightRAG:\n",
        "    \"\"\"Initialize a LightRAG instance backed by the local workspace directory.\"\"\"\n",
        "    rag = LightRAG(\n",
        "        working_dir=LIGHTRAG_WORK_DIR,\n",
        "        embedding_func=openai_embed,\n",
        "        llm_model_func=gpt_4o_mini_complete,\n",
        "    )\n",
        "    await rag.initialize_storages()\n",
        "    # Ensure pipeline_status namespace is initialized so ainsert/aquery work\n",
        "    await initialize_pipeline_status()\n",
        "    return rag\n",
        "\n",
        "\n",
        "rag = await init_lightrag()\n",
        "print(\"LightRAG initialized.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Postgres connectivity OK.\n",
            "Neo4j connectivity OK.\n"
          ]
        }
      ],
      "source": [
        "# Optional: quick connectivity checks for Postgres and Neo4j\n",
        "# Requires `psycopg2` (for Postgres) and `neo4j` Python driver to be installed.\n",
        "\n",
        "import importlib\n",
        "\n",
        "\n",
        "def check_postgres():\n",
        "    if not POSTGRES_DSN:\n",
        "        print(\"POSTGRES_DSN not set; skipping Postgres check.\")\n",
        "        return\n",
        "    try:\n",
        "        psycopg2 = importlib.import_module(\"psycopg2\")\n",
        "    except ImportError:\n",
        "        print(\"psycopg2 is not installed; skipping Postgres check.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with psycopg2.connect(POSTGRES_DSN) as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                cur.execute(\"SELECT 1\")\n",
        "        print(\"Postgres connectivity OK.\")\n",
        "    except Exception as e:\n",
        "        print(\"Postgres connectivity failed:\", e)\n",
        "\n",
        "\n",
        "def check_neo4j():\n",
        "    if not (NEO4J_URI and NEO4J_USER and NEO4J_PASSWORD):\n",
        "        print(\"NEO4J_* env vars not fully set; skipping Neo4j check.\")\n",
        "        return\n",
        "    try:\n",
        "        neo4j = importlib.import_module(\"neo4j\")\n",
        "    except ImportError:\n",
        "        print(\"neo4j Python driver is not installed; skipping Neo4j check.\")\n",
        "        return\n",
        "\n",
        "    from neo4j import GraphDatabase\n",
        "\n",
        "    try:\n",
        "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "        with driver.session() as session:\n",
        "            session.run(\"RETURN 1\").single()\n",
        "        print(\"Neo4j connectivity OK.\")\n",
        "    except Exception as e:\n",
        "        print(\"Neo4j connectivity failed:\", e)\n",
        "\n",
        "\n",
        "check_postgres()\n",
        "check_neo4j()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1660 contexts from transcripts/transcripts_segments.jsonl.\n",
            "Example context:\n",
            " [video_id=video1;start=0.0;end=29.98;segment_id=0] Vi pr√∏ver √• ta vare p√• disse opptakene og hva som er viktig √• gj√∏re. ...\n"
          ]
        }
      ],
      "source": [
        "# Build LightRAG-ready contexts from the timestamped transcript segments\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "transcript_jsonl_path = Path(\"transcripts/transcripts_segments.jsonl\")\n",
        "\n",
        "contexts: list[str] = []\n",
        "contexts_meta: list[dict] = []\n",
        "\n",
        "if not transcript_jsonl_path.exists():\n",
        "    raise FileNotFoundError(f\"Transcript file not found: {transcript_jsonl_path}\")\n",
        "\n",
        "with transcript_jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        rec = json.loads(line)\n",
        "        text = (rec.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        meta = {\n",
        "            \"video_id\": rec.get(\"video_id\"),\n",
        "            \"video_path\": rec.get(\"video_path\"),\n",
        "            \"segment_id\": rec.get(\"segment_id\"),\n",
        "            \"start\": rec.get(\"start\"),\n",
        "            \"end\": rec.get(\"end\"),\n",
        "        }\n",
        "\n",
        "        header = (\n",
        "            f\"[video_id={meta['video_id']};start={meta['start']};\"\n",
        "            f\"end={meta['end']};segment_id={meta['segment_id']}] \"\n",
        "        )\n",
        "        context = header + text\n",
        "\n",
        "        meta[\"text\"] = text\n",
        "        meta[\"context\"] = context\n",
        "\n",
        "        contexts.append(context)\n",
        "        contexts_meta.append(meta)\n",
        "\n",
        "print(f\"Loaded {len(contexts)} contexts from {transcript_jsonl_path}.\")\n",
        "if contexts:\n",
        "    print(\"Example context:\\n\", contexts[0][:300], \"...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring document ID (already exists): doc-879378d49c3aba1929a7b43d4e9914f9 (unknown_source)\n",
            "WARNING: No new unique documents were found.\n",
            "INFO: No documents to process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserting 1660 contexts into LightRAG (this may take a while)...\n",
            "LightRAG insertion complete.\n"
          ]
        }
      ],
      "source": [
        "# Insert the prepared contexts into LightRAG\n",
        "\n",
        "if not contexts:\n",
        "    raise ValueError(\"No contexts loaded; run the previous cell first.\")\n",
        "\n",
        "print(f\"Inserting {len(contexts)} contexts into LightRAG (this may take a while)...\")\n",
        "\n",
        "# We join the contexts into a single large document so LightRAG can re-chunk as needed.\n",
        "joined_contexts = \"\\n\\n\".join(contexts)\n",
        "\n",
        "await rag.ainsert(joined_contexts)\n",
        "\n",
        "print(\"LightRAG insertion complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Embedding func: 8 new workers initialized (Timeouts: Func: 30s, Worker: 60s, Health Check: 75s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing embeddings for contexts (one-time)...\n"
          ]
        }
      ],
      "source": [
        "# Semantic search helper: find relevant segments and return video/timestamp metadata\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Base URL for building clickable links; adjust to your eventual video player.\n",
        "VIDEO_BASE_URL = os.getenv(\"VIDEO_BASE_URL\")\n",
        "\n",
        "# Pre-compute embeddings for all contexts using the same embedding function as LightRAG\n",
        "print(\"Computing embeddings for contexts (one-time)...\")\n",
        "context_embeddings = await rag.embedding_func(contexts)\n",
        "context_embeddings = np.array(context_embeddings, dtype=\"float32\")\n",
        "context_norms = np.linalg.norm(context_embeddings, axis=1) + 1e-8\n",
        "\n",
        "\n",
        "async def search_segments_async(query: str, top_k: int = 5):\n",
        "    \"\"\"Semantic search over transcript segments.\n",
        "\n",
        "    Returns a list of dicts with video_id, start, end, text, score, and a URL\n",
        "    you can later hook up to a player.\n",
        "    \"\"\"\n",
        "\n",
        "    query_emb = await rag.embedding_func([query])\n",
        "    query_emb = np.array(query_emb, dtype=\"float32\")[0]\n",
        "    query_norm = np.linalg.norm(query_emb) + 1e-8\n",
        "\n",
        "    # Cosine similarity between query and all context embeddings\n",
        "    sims = (context_embeddings @ query_emb) / (context_norms * query_norm)\n",
        "    top_idx = np.argsort(-sims)[:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_idx:\n",
        "        meta = contexts_meta[int(idx)].copy()\n",
        "        meta[\"score\"] = float(sims[idx])\n",
        "        # Build a simple URL that encodes video + start time\n",
        "        meta[\"url\"] = f\"{VIDEO_BASE_URL}/{meta['video_id']}?t={int(meta['start'])}\"\n",
        "        results.append(meta)\n",
        "\n",
        "    # Pretty-print results for quick inspection\n",
        "    for r in results:\n",
        "        print(\n",
        "            f\"- video_id={r['video_id']} start={r['start']:.2f}s end={r['end']:.2f}s score={r['score']:.3f}\"\n",
        "        )\n",
        "        print(f\"  url: {r['url']}\")\n",
        "        print(f\"  text: {r['text'][:200]}...\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage in a notebook cell:\n",
        "#   await search_segments_async(\"data sharing platform\", top_k=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote 1660 records to /Users/adi/digdir-video-ai/transcripts/chunks_for_db.jsonl.\n",
            "You can load this JSONL into Postgres, Neo4j, or another vector DB as needed.\n"
          ]
        }
      ],
      "source": [
        "# Export chunks + metadata for external vector/graph stores (PostgreSQL, Neo4j, etc.)\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"transcripts\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "export_jsonl_path = output_dir / \"chunks_for_db.jsonl\"\n",
        "\n",
        "records_written = 0\n",
        "with export_jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in contexts_meta:\n",
        "        rec = {\n",
        "            \"video_id\": meta[\"video_id\"],\n",
        "            \"segment_id\": meta[\"segment_id\"],\n",
        "            \"start\": meta[\"start\"],\n",
        "            \"end\": meta[\"end\"],\n",
        "            \"text\": meta[\"text\"],\n",
        "            # Same URL pattern used in search_segments_async\n",
        "            \"url\": f\"{VIDEO_BASE_URL}/{meta['video_id']}?t={int(meta['start'])}\",\n",
        "        }\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "        records_written += 1\n",
        "\n",
        "print(f\"Wrote {records_written} records to {export_jsonl_path.resolve()}.\")\n",
        "print(\"You can load this JSONL into Postgres, Neo4j, or another vector DB as needed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1660 records from transcripts/chunks_for_db.jsonl.\n",
            "Inserted/updated 1660 rows into video_segments.\n",
            "You can now attach pgvector or another embedding column if desired.\n"
          ]
        }
      ],
      "source": [
        "# Load exported chunks into PostgreSQL (vector-ready table)\n",
        "\n",
        "import json\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "\n",
        "if not POSTGRES_DSN:\n",
        "    raise RuntimeError(\"POSTGRES_DSN is not set. Export POSTGRES_DSN before running this cell.\")\n",
        "\n",
        "try:\n",
        "    psycopg2 = importlib.import_module(\"psycopg2\")\n",
        "except ImportError:\n",
        "    raise ImportError(\"psycopg2 is not installed. Install it with `%pip install psycopg2-binary`.\")\n",
        "\n",
        "export_jsonl_path = Path(\"transcripts/chunks_for_db.jsonl\")\n",
        "if not export_jsonl_path.exists():\n",
        "    raise FileNotFoundError(f\"Export file not found: {export_jsonl_path}\")\n",
        "\n",
        "records: list[dict] = []\n",
        "with export_jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(records)} records from {export_jsonl_path}.\")\n",
        "\n",
        "with psycopg2.connect(POSTGRES_DSN) as conn:\n",
        "    with conn.cursor() as cur:\n",
        "        # Create a simple table for video segments (one row per chunk)\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS video_segments (\n",
        "                id SERIAL PRIMARY KEY,\n",
        "                video_id TEXT NOT NULL,\n",
        "                segment_id INTEGER,\n",
        "                start DOUBLE PRECISION,\n",
        "                \"end\" DOUBLE PRECISION,\n",
        "                text TEXT,\n",
        "                url TEXT,\n",
        "                UNIQUE (video_id, segment_id, start)\n",
        "            );\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        insert_sql = \"\"\"\n",
        "            INSERT INTO video_segments (video_id, segment_id, start, \"end\", text, url)\n",
        "            VALUES (%(video_id)s, %(segment_id)s, %(start)s, %(end)s, %(text)s, %(url)s)\n",
        "            ON CONFLICT (video_id, segment_id, start) DO UPDATE\n",
        "            SET \"end\" = EXCLUDED.\"end\", text = EXCLUDED.text, url = EXCLUDED.url;\n",
        "        \"\"\"\n",
        "        cur.executemany(insert_sql, records)\n",
        "    conn.commit()\n",
        "\n",
        "print(f\"Inserted/updated {len(records)} rows into video_segments.\")\n",
        "print(\"You can now attach pgvector or another embedding column if desired.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1660 records from transcripts/chunks_for_db.jsonl.\n",
            "Upserted 1660 VideoSegment nodes (and Video parents) into Neo4j.\n"
          ]
        }
      ],
      "source": [
        "# Load exported chunks into Neo4j as Video / VideoSegment graph\n",
        "\n",
        "import json\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "\n",
        "if not (NEO4J_URI and NEO4J_USER and NEO4J_PASSWORD):\n",
        "    raise RuntimeError(\"NEO4J_URI / NEO4J_USER / NEO4J_PASSWORD must be set before running this cell.\")\n",
        "\n",
        "try:\n",
        "    neo4j_module = importlib.import_module(\"neo4j\")\n",
        "except ImportError:\n",
        "    raise ImportError(\"neo4j Python driver is not installed. Install it with `%pip install neo4j`.\\n\")\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "export_jsonl_path = Path(\"transcripts/chunks_for_db.jsonl\")\n",
        "if not export_jsonl_path.exists():\n",
        "    raise FileNotFoundError(f\"Export file not found: {export_jsonl_path}\")\n",
        "\n",
        "records: list[dict] = []\n",
        "with export_jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(records)} records from {export_jsonl_path}.\")\n",
        "\n",
        "\n",
        "def _ingest_segment_tx(tx, rec: dict):\n",
        "    tx.run(\n",
        "        \"\"\"\n",
        "        MERGE (v:Video {video_id: $video_id})\n",
        "        MERGE (s:VideoSegment {\n",
        "            video_id: $video_id,\n",
        "            segment_id: $segment_id,\n",
        "            start: $start\n",
        "        })\n",
        "        SET s.end = $end,\n",
        "            s.text = $text,\n",
        "            s.url = $url\n",
        "        MERGE (v)-[:HAS_SEGMENT]->(s)\n",
        "        \"\"\",\n",
        "        **rec,\n",
        "    )\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "with driver.session() as session:\n",
        "    for rec in records:\n",
        "        session.execute_write(_ingest_segment_tx, rec)\n",
        "\n",
        "driver.close()\n",
        "\n",
        "print(f\"Upserted {len(records)} VideoSegment nodes (and Video parents) into Neo4j.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- video_id=video2 start=3057.10s end=3061.10s score=0.591\n",
            "  url: file:///Users/adi/digdir-video-ai/videos/video2?t=3057\n",
            "  text: Vi snakker om datamesh, vi snakker om datafabrik og s√• videre....\n",
            "\n",
            "- video_id=video2 start=3027.10s end=3033.10s score=0.574\n",
            "  url: file:///Users/adi/digdir-video-ai/videos/video2?t=3027\n",
            "  text: Det som er viktig for oss er at vi f√•r en plattform og har en plattform som vi kan bygge videre p√•....\n",
            "\n",
            "- video_id=video2 start=3039.10s end=3057.10s score=0.567\n",
            "  url: file:///Users/adi/digdir-video-ai/videos/video2?t=3039\n",
            "  text: s√• tror vi jo veldig p√• at vi kan utvide funksjonaliteten og gj√∏re den plattformen egnet for fremtidige eller moderne dataarkitekturer fremover....\n",
            "\n",
            "- video_id=video1 start=2231.58s end=2232.58s score=0.548\n",
            "  url: file:///Users/adi/digdir-video-ai/videos/video1?t=2231\n",
            "  text: det er √• dele datadelen....\n",
            "\n",
            "- video_id=video2 start=2443.10s end=2446.10s score=0.539\n",
            "  url: file:///Users/adi/digdir-video-ai/videos/video2?t=2443\n",
            "  text: og det som faktisk skjer nede p√• gulvet hos de som utvikler....\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example semantic query\n",
        "query = \"N√•r snakker de om datadeling og plattform?\"\n",
        "results = await search_segments_async(query, top_k=5)\n",
        "\n",
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('video2',\n",
              " 3027.1,\n",
              " 'Det som er viktig for oss er at vi f√•r en plattform og har en plattform som vi kan bygge videre p√•.')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hit = results[1]  # or any index\n",
        "hit[\"video_id\"], hit[\"start\"], hit[\"text\"][:120]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from pathlib import Path\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "\n",
        "def show_video_segment(video_path: str, start: float, width: int = 640):\n",
        "    \"\"\"Embed a local video in the notebook and seek to `start` seconds.\n",
        "\n",
        "    Works best when the video is inside the project directory (e.g. `videos/video2.mp4`).\n",
        "    If an absolute path is passed, it will be made relative to the current working directory\n",
        "    when possible so Jupyter can serve the file.\n",
        "\n",
        "    Uses a unique DOM id per call so multiple outputs don't conflict, and also\n",
        "    prints the path and timestamp (hh:mm:ss) for manual seeking.\n",
        "    \"\"\"\n",
        "    p = Path(video_path)\n",
        "    try:\n",
        "        # Try to make path relative to notebook root so Jupyter can serve it\n",
        "        p_rel = p.relative_to(Path(os.getcwd()))\n",
        "    except ValueError:\n",
        "        # Fall back to original path\n",
        "        p_rel = p\n",
        "\n",
        "    src = p_rel.as_posix()\n",
        "    elem_id = f\"segment_player_{uuid.uuid4().hex}\"\n",
        "    start_sec = int(start)\n",
        "    h = start_sec // 3600\n",
        "    m = (start_sec % 3600) // 60\n",
        "    s = start_sec % 60\n",
        "    ts = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "    # Also print to the notebook output for manual use\n",
        "    print(f\"Video: {src} | Start: {ts} ({start_sec}s)\")\n",
        "\n",
        "    return HTML(f\"\"\"\n",
        "    <p><strong>Video:</strong> {src}<br><strong>Start:</strong> {ts} ({start_sec}s)</p>\n",
        "    <video id=\"{elem_id}\" width=\"{width}\" controls>\n",
        "      <source src=\"{src}\" type=\"video/mp4\">\n",
        "      Your browser does not support the video tag.\n",
        "    </video>\n",
        "    <script>\n",
        "      (function() {{\n",
        "        const v = document.getElementById('{elem_id}');\n",
        "        if (!v) return;\n",
        "        const seekTo = {start_sec};\n",
        "        function seek() {{\n",
        "          try {{ v.currentTime = seekTo; }} catch (e) {{}}\n",
        "        }}\n",
        "        if (v.readyState >= 1) {{\n",
        "          // Metadata already loaded\n",
        "          seek();\n",
        "        }} else {{\n",
        "          v.addEventListener('loadedmetadata', () => {{\n",
        "            seek();\n",
        "          }}, {{ once: true }});\n",
        "        }}\n",
        "      }})();\n",
        "    </script>\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <video id=\"segment_player_21fb1b6dc0cb4e2c8b688f6a2d71b6d7\" width=\"640\" controls>\n",
              "      <source src=\"videos/video2.mp4\" type=\"video/mp4\">\n",
              "      Your browser does not support the video tag.\n",
              "    </video>\n",
              "    <script>\n",
              "      const v = document.getElementById('segment_player_21fb1b6dc0cb4e2c8b688f6a2d71b6d7');\n",
              "      if (v) {\n",
              "        v.addEventListener('loadedmetadata', () => {\n",
              "          v.currentTime = 3027;\n",
              "        });\n",
              "      }\n",
              "    </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hit = results[1]\n",
        "show_video_segment(hit[\"video_path\"], hit[\"start\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
